{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2780b-eb4c-4098-912b-da18099822e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing, radius_graph, knn_graph\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "from torch_scatter.scatter import scatter\n",
    "from torch_scatter import scatter_min, scatter_max\n",
    "from torch_cluster import knn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_cluster import fps\n",
    "import pandas as pd\n",
    "from models.gpgin import ConvLayer, MLP\n",
    "from torch.nn.functional import silu\n",
    "from torch_geometric.typing import (\n",
    "    Adj,\n",
    "    OptTensor,\n",
    "    SparseTensor,\n",
    "    pyg_lib,\n",
    "    torch_sparse,\n",
    ")\n",
    "from typing import Tuple, List, Dict, Union\n",
    "from pprint import pprint\n",
    "from torch_geometric.data import Data, DataListLoader, Dataset, InMemoryDataset, Batch\n",
    "from torch_geometric.loader import DataListLoader, DataLoader\n",
    "from torch_geometric.nn import *\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch, add_self_loops, remove_self_loops\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "import torch\n",
    "from torch import nn\n",
    "import rdkit\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import RDLogger\n",
    "\n",
    "from copy import deepcopy\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple, List, Dict, Union\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import MessagePassing, radius_graph\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.typing import (\n",
    "    Adj,\n",
    "    OptTensor,\n",
    "    SparseTensor,\n",
    "    pyg_lib,\n",
    "    torch_sparse,\n",
    ")\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    "    extract_zip,\n",
    ")\n",
    "from rdkit import Chem\n",
    "import os\n",
    "# Suppress RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "cuda=torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "import sascorer\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "from models import *\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "from typing import List\n",
    "from prolog import *\n",
    "import torchlens as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca76cd74-089b-4463-a72a-79cafed71c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sup=Chem.SDMolSupplier(\"datasets/qm9/raw/gdb9.sdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c62370c0-29b8-4dac-9408-ab17d0808ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\labio\\micromamba\\envs\\biocheminformatics\\Lib\\site-packages\\torch_geometric\\io\\fs.py:229: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([MolData])` to allowlist this global.\n",
      "  warnings.warn(f\"{warn_msg} Please use \"\n"
     ]
    }
   ],
   "source": [
    "qm9=QM9(\"datasets/qm9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df22dc7-0c0c-4a34-82e0-3e89a5f06e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolData(edge_index=[2, 1], pos=[1, 3], atom_type=[1], edge_type=[1], dist=[1, 1], D=[1, 3], U=[1, 3], V=[1, 3], logp=[1], u298_atom=[1], h298_atom=[1], r2=[1], u0=[1], lumo=[1], alpha=[1], cv=[1], B=[1], zpve=[1], A=[1], u298=[1], C=[1], gap=[1], h298=[1], homo=[1], u0_atom=[1], g298_atom=[1], mu=[1], g298=[1], dest=[0], inbound=[0, 2], ang_deltas=[0], anchor_ang=[0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c3aec5-3f68-464d-8341-f4cb8487e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE=1024\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(qm9, [len(qm9)-TEST_SIZE, TEST_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd9e0f9-203b-4e30-8e92-108be3daac0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x226c1815fc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_idx=100_000\n",
    "sample=qm9[sample_idx]\n",
    "sup[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1be3ce0-a029-4803-943d-dd1c26be9822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/3dmoljs_load.v0": "<div id=\"3dmolviewer_1750327397151164\"  style=\"position: relative; width: 200px; height: 200px;\">\n        <p id=\"3dmolwarning_1750327397151164\" style=\"background-color:#ffcccc;color:black\">3Dmol.js failed to load for some reason.  Please check your browser console for error messages.<br></p>\n        </div>\n<script>\n\nvar loadScriptAsync = function(uri){\n  return new Promise((resolve, reject) => {\n    //this is to ignore the existence of requirejs amd\n    var savedexports, savedmodule;\n    if (typeof exports !== 'undefined') savedexports = exports;\n    else exports = {}\n    if (typeof module !== 'undefined') savedmodule = module;\n    else module = {}\n\n    var tag = document.createElement('script');\n    tag.src = uri;\n    tag.async = true;\n    tag.onload = () => {\n        exports = savedexports;\n        module = savedmodule;\n        resolve();\n    };\n  var firstScriptTag = document.getElementsByTagName('script')[0];\n  firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n});\n};\n\nif(typeof $3Dmolpromise === 'undefined') {\n$3Dmolpromise = null;\n  $3Dmolpromise = loadScriptAsync('https://cdnjs.cloudflare.com/ajax/libs/3Dmol/2.4.2/3Dmol-min.js');\n}\n\nvar viewer_1750327397151164 = null;\nvar warn = document.getElementById(\"3dmolwarning_1750327397151164\");\nif(warn) {\n    warn.parentNode.removeChild(warn);\n}\n$3Dmolpromise.then(function() {\nviewer_1750327397151164 = $3Dmol.createViewer(document.getElementById(\"3dmolviewer_1750327397151164\"),{backgroundColor:\"white\"});\nviewer_1750327397151164.zoomTo();\n\tviewer_1750327397151164.addModel(\"gdb_100001\\n     RDKit          3D\\n\\n  9  8  0  0  1  0  0  0  0  0999 V2000\\n   -0.1724    1.5875   -0.0819 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.0389    0.0660    0.0618 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.8114   -0.4535    1.2530 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.2423    0.1167    1.1860 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.1553   -0.0259    2.5821 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.8080   -2.0234    1.1873 C   0  0  1  0  0  0  0  0  0  0  0  0\\n    1.6200   -2.6366    0.0287 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.3182   -2.6559    2.4696 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.6737   -3.3785    3.1865 O   0  0  0  0  0  0  0  0  0  0  0  0\\n  1  2  1  0\\n  6  7  1  6\\n  2  3  1  0\\n  3  4  1  0\\n  3  6  1  0\\n  6  8  1  0\\n  3  5  1  0\\n  8  9  2  0\\nM  END\\n\",\"mol\");\n\tviewer_1750327397151164.setStyle({\"stick\": {}});\n\tviewer_1750327397151164.zoomTo();\nviewer_1750327397151164.render();\n});\n</script>",
      "text/html": [
       "<div id=\"3dmolviewer_1750327397151164\"  style=\"position: relative; width: 200px; height: 200px;\">\n",
       "        <p id=\"3dmolwarning_1750327397151164\" style=\"background-color:#ffcccc;color:black\">3Dmol.js failed to load for some reason.  Please check your browser console for error messages.<br></p>\n",
       "        </div>\n",
       "<script>\n",
       "\n",
       "var loadScriptAsync = function(uri){\n",
       "  return new Promise((resolve, reject) => {\n",
       "    //this is to ignore the existence of requirejs amd\n",
       "    var savedexports, savedmodule;\n",
       "    if (typeof exports !== 'undefined') savedexports = exports;\n",
       "    else exports = {}\n",
       "    if (typeof module !== 'undefined') savedmodule = module;\n",
       "    else module = {}\n",
       "\n",
       "    var tag = document.createElement('script');\n",
       "    tag.src = uri;\n",
       "    tag.async = true;\n",
       "    tag.onload = () => {\n",
       "        exports = savedexports;\n",
       "        module = savedmodule;\n",
       "        resolve();\n",
       "    };\n",
       "  var firstScriptTag = document.getElementsByTagName('script')[0];\n",
       "  firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n",
       "});\n",
       "};\n",
       "\n",
       "if(typeof $3Dmolpromise === 'undefined') {\n",
       "$3Dmolpromise = null;\n",
       "  $3Dmolpromise = loadScriptAsync('https://cdnjs.cloudflare.com/ajax/libs/3Dmol/2.4.2/3Dmol-min.js');\n",
       "}\n",
       "\n",
       "var viewer_1750327397151164 = null;\n",
       "var warn = document.getElementById(\"3dmolwarning_1750327397151164\");\n",
       "if(warn) {\n",
       "    warn.parentNode.removeChild(warn);\n",
       "}\n",
       "$3Dmolpromise.then(function() {\n",
       "viewer_1750327397151164 = $3Dmol.createViewer(document.getElementById(\"3dmolviewer_1750327397151164\"),{backgroundColor:\"white\"});\n",
       "viewer_1750327397151164.zoomTo();\n",
       "\tviewer_1750327397151164.addModel(\"gdb_100001\\n     RDKit          3D\\n\\n  9  8  0  0  1  0  0  0  0  0999 V2000\\n   -0.1724    1.5875   -0.0819 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.0389    0.0660    0.0618 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.8114   -0.4535    1.2530 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.2423    0.1167    1.1860 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.1553   -0.0259    2.5821 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.8080   -2.0234    1.1873 C   0  0  1  0  0  0  0  0  0  0  0  0\\n    1.6200   -2.6366    0.0287 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.3182   -2.6559    2.4696 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.6737   -3.3785    3.1865 O   0  0  0  0  0  0  0  0  0  0  0  0\\n  1  2  1  0\\n  6  7  1  6\\n  2  3  1  0\\n  3  4  1  0\\n  3  6  1  0\\n  6  8  1  0\\n  3  5  1  0\\n  8  9  2  0\\nM  END\\n\",\"mol\");\n",
       "\tviewer_1750327397151164.setStyle({\"stick\": {}});\n",
       "\tviewer_1750327397151164.zoomTo();\n",
       "viewer_1750327397151164.render();\n",
       "});\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example\n",
    "show(sup[sample_idx])  # or 'P'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c136430-aa5e-4c51-aa38-74ea863a90cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingContext:\n",
    "    def __init__(self, cls, *args, **kwargs):\n",
    "        self.inner = cls(*args, **kwargs).to(cuda)\n",
    "        self.name = cls.__name__\n",
    "        self.optim = torch.optim.AdamW(self.inner.parameters())\n",
    "        self.sched = torch.optim.lr_scheduler.ExponentialLR(self.optim, gamma=0.98)\n",
    "        self.num_parameters = sum(map(torch.numel,self.inner.parameters()))\n",
    "        self.train_loss_record = dict()\n",
    "        self.test_loss_record = dict()\n",
    "        self.results = list()\n",
    "        self.total_iters = 0\n",
    "        self.running_loss = 0\n",
    "        self.best_eval_loss = 999999\n",
    "        self.stopped = False\n",
    "        self.batch_size=32\n",
    "        self.train_loss_metric='MSE'\n",
    "        self.eval_loss_metric='MAE'\n",
    "        self.last_target_name='normalized_u0'\n",
    "        self.last_dataset_name='QM9'\n",
    "        self.training=True\n",
    "    def save(self, prefix='saves'):\n",
    "        save_model(\n",
    "            self.name,\n",
    "            self.inner,\n",
    "            optimizer=self.optim,\n",
    "            scheduler=self.sched,\n",
    "            loss_record={\n",
    "                'train':self.train_loss_record,\n",
    "                'test':self.test_loss_record,\n",
    "            },\n",
    "            total_training_iters=self.total_iters,\n",
    "            last_batch_size=self.batch_size,\n",
    "            loss_metric={\n",
    "                'train':'MSE',\n",
    "                'test':'MAE',\n",
    "            },\n",
    "            last_target_name=self.last_target_name,\n",
    "            last_dataset_name=self.last_dataset_name\n",
    "        )\n",
    "    @classmethod\n",
    "    def load(cls, name, class_, prefix='saves', training=False, override=None):\n",
    "        if override is None:\n",
    "            override=dict()\n",
    "        checkpoint=torch.load(os.path.join(prefix,name,'checkpoint.pth'))\n",
    "        self=cls(class_,**dict(**checkpoint['config'],**override))\n",
    "        self.inner.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.total_iters=checkpoint['total_training_iters']\n",
    "        self.batch_size=checkpoint['last_batch_size']\n",
    "        self.train_loss_record=checkpoint['loss_record']['train']\n",
    "        self.test_loss_record=checkpoint['loss_record']['test']\n",
    "        self.best_eval_loss=min(self.test_loss_record.values())\n",
    "        self.training=training\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d0c39b-2663-48f3-ae16-25e7ed82b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def auto_save_hyperparams(init_fn):\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        # Bind the arguments to the function signature and apply defaults\n",
    "        sig = inspect.signature(init_fn)\n",
    "        bound_args = sig.bind(self, *args, **kwargs)\n",
    "        bound_args.apply_defaults()\n",
    "        # Save all parameters except 'self'\n",
    "        self.hparams = {\n",
    "            name: value \n",
    "            for name, value in bound_args.arguments.items() \n",
    "            if name != \"self\"\n",
    "        }\n",
    "        return init_fn(self, *args, **kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53cd4483-64ea-4c4a-89c9-e6bd40183de6",
   "metadata": {},
   "source": [
    "import inspect\n",
    "\n",
    "def auto_save_hyperparams(init_fn):\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        # Bind the arguments to the function signature and apply defaults\n",
    "        sig = inspect.signature(init_fn)\n",
    "        bound_args = sig.bind(self, *args, **kwargs)\n",
    "        bound_args.apply_defaults()\n",
    "        # Save all parameters except 'self'\n",
    "        self.hparams = {\n",
    "            name: value \n",
    "            for name, value in bound_args.arguments.items() \n",
    "            if name != \"self\"\n",
    "        }\n",
    "        return init_fn(self, *args, **kwargs)\n",
    "    return wrapper\n",
    "    \n",
    "class DownSampler(nn.Module):\n",
    "    def __init__(self,\n",
    "            ratio,\n",
    "            mlp_dims_node: List[int],\n",
    "            mlp_dims_edge: List[int],\n",
    "            \n",
    "            aggr: str = 'sum',\n",
    "            \n",
    "            node_norm=nn.LayerNorm, \n",
    "            final_node_norm=nn.Identity, \n",
    "            \n",
    "            edge_norm=nn.LayerNorm,\n",
    "            final_edge_norm=nn.Identity, \n",
    "            \n",
    "            activation=nn.SiLU, \n",
    "            final_activation=nn.Identity, \n",
    "            \n",
    "            dropout_rate=0.1, \n",
    "            final_dropout_rate=0.0,\n",
    "            q=1,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"ratio\",torch.tensor(ratio))\n",
    "        self.register_buffer(\"q\",torch.tensor(q))\n",
    "        self.conv = ConvLayer(\n",
    "            mlp_dims_node=mlp_dims_node,\n",
    "            mlp_dims_edge=mlp_dims_edge,\n",
    "            aggr=aggr,\n",
    "            node_norm=node_norm,\n",
    "            final_node_norm=final_node_norm,\n",
    "            edge_norm=edge_norm,\n",
    "            final_edge_norm=final_edge_norm,\n",
    "            activation=activation,\n",
    "            final_activation=final_activation,\n",
    "            dropout_rate=dropout_rate,\n",
    "            final_dropout_rate=final_dropout_rate,\n",
    "        )\n",
    "        self.layer_norm=nn.LayerNorm(mlp_dims_edge[-1])\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.conv.reset_parameters()\n",
    "        \n",
    "    def forward(self, K, V, batch=None, dK_scale=1.0, target_points=None, edge_index=None, sampled_idx=None):\n",
    "        #K float:     N, k_dim\n",
    "        #V float:     N, v_dim\n",
    "        #batch int: N\n",
    "        if batch is None:\n",
    "            batch=torch.zeros(K.shape[-2],device=K.device,dtype=torch.long)\n",
    "        if edge_index is None or sampled_idx is None:\n",
    "            if sampled_idx is None:\n",
    "                sampled_idx = fps(\n",
    "                    K, \n",
    "                    ratio=self.ratio, \n",
    "                    batch=batch, \n",
    "                    random_start=True\n",
    "                )\n",
    "                edge_idx = knn(K[sampled_idx], K, 1, batch_x=batch[sampled_idx], batch_y=batch).flip(0)\n",
    "                _, sampled_idx = scatter_min(torch.rand(edge_idx.shape[1],device=K.device), edge_idx[0], dim=0)\n",
    "            # clamp k to [1, N] so we never ask for more neighbors than exist\n",
    "            N = K.size(0)\n",
    "            q = int(self.q.item())\n",
    "            k_eff = max(1, min(q, N))\n",
    "            edge_index = knn(\n",
    "                K[sampled_idx],      # src\n",
    "                K,                   # tgt\n",
    "                k_eff, \n",
    "                batch_x=batch[sampled_idx], \n",
    "                batch_y=batch\n",
    "            )\n",
    "        src, tgt = edge_index\n",
    "        dK = (K[sampled_idx][tgt]-K[src]).norm(2,-1).unsqueeze(-1)*dK_scale\n",
    "        pooledV = self.conv(\n",
    "            (V, torch.zeros_like(V[sampled_idx]) if target_points is None else target_points), \n",
    "            edge_index, \n",
    "            dK)\n",
    "        return K[sampled_idx], pooledV, batch[sampled_idx], edge_index, sampled_idx\n",
    "\n",
    "class UpSampler(nn.Module):\n",
    "    def __init__(self,\n",
    "            mlp_dims_node: List[int],\n",
    "            mlp_dims_edge: List[int],\n",
    "            \n",
    "            aggr: str = 'sum',\n",
    "            \n",
    "            node_norm=nn.LayerNorm, \n",
    "            final_node_norm=nn.Identity, \n",
    "            \n",
    "            edge_norm=nn.LayerNorm,\n",
    "            final_edge_norm=nn.Identity, \n",
    "            \n",
    "            activation=nn.SiLU, \n",
    "            final_activation=nn.Identity, \n",
    "            \n",
    "            dropout_rate=0.1, \n",
    "            final_dropout_rate=0.0,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.conv = ConvLayer(\n",
    "            mlp_dims_node=mlp_dims_node,\n",
    "            mlp_dims_edge=mlp_dims_edge,\n",
    "            aggr=aggr,\n",
    "            node_norm=node_norm,\n",
    "            final_node_norm=final_node_norm,\n",
    "            edge_norm=edge_norm,\n",
    "            final_edge_norm=final_edge_norm,\n",
    "            activation=activation,\n",
    "            final_activation=final_activation,\n",
    "            dropout_rate=dropout_rate,\n",
    "            final_dropout_rate=final_dropout_rate,\n",
    "        )\n",
    "        self.layer_norm=nn.LayerNorm(mlp_dims_edge[0])\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.conv.reset_parameters()\n",
    "        \n",
    "    def forward(self, K1, K2, V1, V2, edge_index, dK_scale=1.0):\n",
    "        tgt, src = reversed_edge_index = edge_index.flip(0)\n",
    "        dK = (K2[tgt]-K1[src]).norm(2,-1).unsqueeze(-1)*dK_scale\n",
    "        informed_V1 = self.conv(\n",
    "            (V2, V1), \n",
    "            reversed_edge_index, \n",
    "            dK)\n",
    "        return informed_V1\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "            ratios,\n",
    "            mlp_dims_node: List[int],\n",
    "            mlp_dims_edge: List[int],\n",
    "            \n",
    "            aggr: str = 'sum',\n",
    "            \n",
    "            node_norm=nn.LayerNorm, \n",
    "            final_node_norm=nn.Identity, \n",
    "            \n",
    "            edge_norm=nn.LayerNorm,\n",
    "            final_edge_norm=nn.Identity, \n",
    "            \n",
    "            activation=nn.SiLU, \n",
    "            final_activation=nn.Identity, \n",
    "            \n",
    "            dropout_rate=0.1, \n",
    "            final_dropout_rate=0.0,\n",
    "            q=1,\n",
    "            k_dim=2,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"k_dim\",torch.tensor(k_dim))\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        self.register_buffer(\"n_layers\",torch.tensor(len(ratios)))\n",
    "        for ratio in ratios:\n",
    "            down=DownSampler(\n",
    "                ratio=ratio,\n",
    "                mlp_dims_node=mlp_dims_node,\n",
    "                mlp_dims_edge=mlp_dims_edge,\n",
    "                aggr=aggr,\n",
    "                node_norm=node_norm,\n",
    "                final_node_norm=final_node_norm,\n",
    "                edge_norm=edge_norm,\n",
    "                final_edge_norm=final_edge_norm,\n",
    "                activation=activation,\n",
    "                final_activation=final_activation,\n",
    "                dropout_rate=dropout_rate,\n",
    "                final_dropout_rate=final_dropout_rate,\n",
    "                q=q,\n",
    "            )\n",
    "            up=UpSampler(\n",
    "                mlp_dims_node=mlp_dims_node,\n",
    "                mlp_dims_edge=mlp_dims_edge,\n",
    "                aggr=aggr,\n",
    "                node_norm=node_norm,\n",
    "                final_node_norm=final_node_norm,\n",
    "                edge_norm=edge_norm,\n",
    "                final_edge_norm=final_edge_norm,\n",
    "                activation=activation,\n",
    "                final_activation=final_activation,\n",
    "                dropout_rate=dropout_rate,\n",
    "                final_dropout_rate=final_dropout_rate,\n",
    "            )\n",
    "            self.downs.append(down)\n",
    "            self.ups.append(up)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for d in self.downs:\n",
    "            d.reset_parameters()\n",
    "        for u in self.ups:\n",
    "            u.reset_parameters()\n",
    "        \n",
    "    def forward(self, K, V, batch=None, history=None):\n",
    "        \n",
    "        res=[(K, V, batch, None, None)]\n",
    "        if history is None:\n",
    "            history = [(None, None, None, None, None) for _ in self.downs]\n",
    "        dK_scale=torch.tensor(1.0)\n",
    "        for i,(down, hist_entry) in enumerate(zip(self.downs, history)):\n",
    "            hist_K, hist_V, hist_batch, hist_edge_index, hist_sampled_idx = hist_entry\n",
    "            K, V, batch, edge_index, sampled_idx = out = down(\n",
    "                K, V, batch, \n",
    "                dK_scale=dK_scale, \n",
    "                target_points=hist_V, \n",
    "                edge_index=hist_edge_index,\n",
    "                sampled_idx=hist_sampled_idx\n",
    "            )\n",
    "            V=silu(V)\n",
    "            history[i] = (K, V, batch, edge_index, sampled_idx)\n",
    "            dK_scale=dK_scale*(down.ratio).pow(1/self.k_dim)\n",
    "            if i+1<self.n_layers:\n",
    "                res.append(out)\n",
    "        for up, (_K, _V, _, _edge_index, _), down in zip(\n",
    "            self.ups, \n",
    "            reversed(res), \n",
    "            reversed(self.downs)\n",
    "        ):\n",
    "            dK_scale=dK_scale*(down.ratio).pow(-1/self.k_dim)\n",
    "            V=up(_K, K, _V, V, edge_index, dK_scale=dK_scale)\n",
    "            V=silu(V)\n",
    "            K=_K\n",
    "            edge_index=_edge_index\n",
    "        return V, history\n",
    "\n",
    "class QM9Regressor(nn.Module):\n",
    "    @auto_save_hyperparams\n",
    "    def __init__(self,\n",
    "            domain_dim,\n",
    "            raw_feature_dim,\n",
    "            latent_feature_dim,\n",
    "            ratios,\n",
    "            mlp_dims_node: List[int],\n",
    "            mlp_dims_edge: List[int],\n",
    "            \n",
    "            aggr: str = 'sum',\n",
    "            \n",
    "            node_norm=nn.LayerNorm, \n",
    "            final_node_norm=nn.Identity, \n",
    "            \n",
    "            edge_norm=nn.LayerNorm,\n",
    "            final_edge_norm=nn.Identity, \n",
    "            \n",
    "            activation=nn.SiLU, \n",
    "            final_activation=nn.Identity, \n",
    "            \n",
    "            dropout_rate=0.1, \n",
    "            final_dropout_rate=0.0,\n",
    "\n",
    "            q=1,\n",
    "            n_encoders=3,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        assert mlp_dims_node[0]==mlp_dims_node[-1]==mlp_dims_edge[-1]==latent_feature_dim\n",
    "        self.atom_emb = nn.Embedding(200, latent_feature_dim)\n",
    "        self.unets=nn.ModuleList([])\n",
    "        for _ in range(n_encoders):\n",
    "            unet=UNet(\n",
    "                    ratios=ratios,\n",
    "                    mlp_dims_node=mlp_dims_node,\n",
    "                    mlp_dims_edge=mlp_dims_edge,\n",
    "                    aggr=aggr,\n",
    "                    node_norm=node_norm,\n",
    "                    final_node_norm=final_node_norm,\n",
    "                    edge_norm=edge_norm,\n",
    "                    final_edge_norm=final_edge_norm,\n",
    "                    activation=activation,\n",
    "                    final_activation=final_activation,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    final_dropout_rate=final_dropout_rate,\n",
    "                    q=q,\n",
    "                    k_dim=domain_dim,\n",
    "            )\n",
    "            self.unets.append(unet)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for unet in self.unets:\n",
    "            unet.reset_parameters()\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "        \n",
    "    def get_config(self):\n",
    "        return self.hparams\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self._forward(data.pos,data.atom_type,data.batch)\n",
    "    \n",
    "    def _forward(self, K, V, batch=None):\n",
    "        #K: float [N, 3] -> position\n",
    "        #V: int [N] -> atom type\n",
    "        #batch: int [N] -> minibatch id\n",
    "        V=self.atom_emb(V)\n",
    "        history=None\n",
    "        for unet in self.unets:\n",
    "            V, history = unet(K, V, batch, history=history)\n",
    "        V=scatter(V, batch, dim=0, reduce='sum').sum(-1)\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c318033e-9ae8-4d4c-88df-7abccdc2a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_scatter import scatter_min\n",
    "from torch_geometric.nn import fps, knn\n",
    "from torch_geometric.nn.norm import GraphNorm\n",
    "from models.gpgin import GaussianSmearing, ConvLayer  # adjust import paths\n",
    "\n",
    "class DownSampler(nn.Module):\n",
    "    def __init__(self,\n",
    "                 ratio,\n",
    "                 mlp_dims_node, mlp_dims_edge,\n",
    "                 aggr='sum',\n",
    "                 node_norm=nn.LayerNorm, final_node_norm=nn.Identity,\n",
    "                 edge_norm=nn.LayerNorm, final_edge_norm=nn.Identity,\n",
    "                 activation=nn.SiLU, final_activation=nn.Identity,\n",
    "                 dropout_rate=0.1, final_dropout_rate=0.0,\n",
    "                 q=1,\n",
    "                 cutoff=10.0, max_neighbors=32):\n",
    "        super().__init__()\n",
    "        self.register_buffer('ratio', torch.tensor(ratio))\n",
    "        self.register_buffer('q',     torch.tensor(q))\n",
    "        # gaussian smear from 0→cutoff into edge_dims_edge[0] bins\n",
    "        self.gauss = GaussianSmearing(0.0, cutoff, mlp_dims_edge[0])\n",
    "        # message‐passing\n",
    "        self.conv = ConvLayer(\n",
    "            mlp_dims_node, mlp_dims_edge,\n",
    "            aggr=aggr,\n",
    "            node_norm=node_norm, final_node_norm=final_node_norm,\n",
    "            edge_norm=edge_norm, final_edge_norm=final_edge_norm,\n",
    "            activation=activation, final_activation=final_activation,\n",
    "            dropout_rate=dropout_rate, final_dropout_rate=final_dropout_rate,\n",
    "        )\n",
    "        # graph‐norm + residual‐scaling\n",
    "        self.norm = GraphNorm(mlp_dims_node[-1])\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.8))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv.reset_parameters()\n",
    "        nn.init.constant_(self.alpha, 0.8)\n",
    "\n",
    "    def forward(self, K, V, batch=None, dK_scale=1.0,\n",
    "                target_points=None, edge_index=None, sampled_idx=None):\n",
    "        N = K.size(0)\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(N, device=K.device, dtype=torch.long)\n",
    "\n",
    "        # --- sample points if needed ---\n",
    "        if sampled_idx is None:\n",
    "            sampled_idx = fps(K, ratio=self.ratio, batch=batch, random_start=True)\n",
    "            # ensure at least one neighbor per sampled node\n",
    "            edge_idx = knn(K[sampled_idx], K, 1,\n",
    "                           batch_x=batch[sampled_idx], batch_y=batch).flip(0)\n",
    "            _, sampled_idx = scatter_min(torch.rand(edge_idx.size(1), device=K.device),\n",
    "                                         edge_idx[0], dim=0)\n",
    "            sampled_idx = sampled_idx[(0<=sampled_idx)&(sampled_idx<K.shape[0])]\n",
    "        # --- compute effective k and kNN graph ---\n",
    "        q = int(self.q.item())\n",
    "        k_eff = max(1, min(q, N))\n",
    "        edge_index = knn(\n",
    "            K[sampled_idx], K, k_eff,\n",
    "            batch_x=batch[sampled_idx], batch_y=batch\n",
    "        )\n",
    "\n",
    "        src, tgt = edge_index\n",
    "        # raw distances\n",
    "        dists = (K[sampled_idx][tgt] - K[src]).norm(dim=-1)\n",
    "        # gaussian‐smeared edge features\n",
    "        edge_attr = self.gauss(dists)  # [E, edge_dim_0]\n",
    "\n",
    "        # message‐passing\n",
    "        h_old = V[sampled_idx]\n",
    "        h_target = (torch.zeros_like(h_old)\n",
    "                    if target_points is None else target_points)\n",
    "        Δh = self.conv((V, h_target), edge_index, edge_attr * dK_scale)\n",
    "\n",
    "        return (K[sampled_idx],\n",
    "                Δh,\n",
    "                batch[sampled_idx],\n",
    "                edge_index,\n",
    "                sampled_idx)\n",
    "\n",
    "\n",
    "class UpSampler(nn.Module):\n",
    "    def __init__(self,\n",
    "                 mlp_dims_node, mlp_dims_edge,\n",
    "                 aggr='sum',\n",
    "                 node_norm=nn.LayerNorm, final_node_norm=nn.Identity,\n",
    "                 edge_norm=nn.LayerNorm, final_edge_norm=nn.Identity,\n",
    "                 activation=nn.SiLU, final_activation=nn.Identity,\n",
    "                 dropout_rate=0.1, final_dropout_rate=0.0,\n",
    "                 cutoff=10.0, max_neighbors=32):\n",
    "        super().__init__()\n",
    "        self.conv = ConvLayer(\n",
    "            mlp_dims_node, mlp_dims_edge,\n",
    "            aggr=aggr,\n",
    "            node_norm=node_norm, final_node_norm=final_node_norm,\n",
    "            edge_norm=edge_norm, final_edge_norm=final_edge_norm,\n",
    "            activation=activation, final_activation=final_activation,\n",
    "            dropout_rate=dropout_rate, final_dropout_rate=final_dropout_rate,\n",
    "        )\n",
    "        self.gauss = GaussianSmearing(0.0, cutoff, mlp_dims_edge[0])\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.8))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv.reset_parameters()\n",
    "        nn.init.constant_(self.alpha, 0.8)\n",
    "\n",
    "    def forward(self, K_low, K_high, V_low, V_high, edge_index, dK_scale=1.0):\n",
    "        # reverse edge_index so src→tgt is low→high\n",
    "        tgt, src = edge_index.flip(0)\n",
    "        dists = (K_high[tgt] - K_low[src]).norm(dim=-1)\n",
    "        edge_attr = self.gauss(dists) * dK_scale\n",
    "\n",
    "        h_old = V_low\n",
    "        Δh = self.conv((V_high, V_low), edge_index.flip(0), edge_attr)\n",
    "\n",
    "\n",
    "        return Δh\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, ratios,\n",
    "                 mlp_dims_node, mlp_dims_edge,\n",
    "                 aggr='sum',\n",
    "                 node_norm=nn.LayerNorm, final_node_norm=nn.Identity,\n",
    "                 edge_norm=nn.LayerNorm, final_edge_norm=nn.Identity,\n",
    "                 activation=nn.SiLU, final_activation=nn.Identity,\n",
    "                 dropout_rate=0.1, final_dropout_rate=0.0,\n",
    "                 q=1, k_dim=2, cutoff=10.0, max_neighbors=32):\n",
    "        super().__init__()\n",
    "        self.k_dim = k_dim\n",
    "        self.activation = activation()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups   = nn.ModuleList()\n",
    "        self.down_norms = nn.ModuleList()\n",
    "        self.up_norms = nn.ModuleList()\n",
    "        for ratio in ratios:\n",
    "            self.downs.append(DownSampler(\n",
    "                ratio,\n",
    "                mlp_dims_node, mlp_dims_edge,\n",
    "                aggr, node_norm, final_node_norm,\n",
    "                edge_norm, final_edge_norm,\n",
    "                activation, final_activation,\n",
    "                dropout_rate, final_dropout_rate,\n",
    "                q, cutoff, max_neighbors\n",
    "            ))\n",
    "            self.ups.append(UpSampler(\n",
    "                mlp_dims_node, mlp_dims_edge,\n",
    "                aggr, node_norm, final_node_norm,\n",
    "                edge_norm, final_edge_norm,\n",
    "                activation, final_activation,\n",
    "                dropout_rate, final_dropout_rate,\n",
    "                cutoff, max_neighbors\n",
    "            ))\n",
    "        self.reset_parameters()\n",
    "        for _ in self.ups:\n",
    "            self.down_norms.append(GraphNorm(mlp_dims_node[-1]))\n",
    "        for _ in self.downs:\n",
    "            self.up_norms.append(GraphNorm(mlp_dims_node[-1]))\n",
    "    def reset_parameters(self):\n",
    "        for d in self.downs: d.reset_parameters()\n",
    "        for u in self.ups:   u.reset_parameters()\n",
    "\n",
    "    def forward(self, K, V, batch=None, history=None, is_last=False):\n",
    "        # identical logic as before, but uses the new DownSampler/UpSampler\n",
    "        if history is None:\n",
    "            history = [[None,]*5 for _ in range(1+len(self.downs))]\n",
    "            history[0] = [K, V, batch, None, None]\n",
    "        dK_scale = torch.tensor(1.0, device=K.device)\n",
    "        # DOWN\n",
    "        for i, (down, norm) in enumerate(zip(self.downs, self.down_norms)):\n",
    "            _, target_points, _, edge_index, sampled_idx = history[i+1]\n",
    "            K, V_, batch, edge_index, sampled_idx = down(\n",
    "                K, V, batch,\n",
    "                dK_scale=dK_scale,\n",
    "                target_points=target_points,\n",
    "                edge_index=edge_index,\n",
    "                sampled_idx=sampled_idx\n",
    "            )\n",
    "            \n",
    "            # print(\"down norm V_ batch:\",V_.shape,batch.shape)\n",
    "            V_=norm(V_,batch)\n",
    "            V_=self.activation(V_)\n",
    "            if target_points is not None:\n",
    "                V =target_points+down.alpha*V_\n",
    "            else:\n",
    "                V = V_\n",
    "            history[i+1] = [K, V, batch, edge_index, sampled_idx]\n",
    "            dK_scale = dK_scale * (down.ratio).pow(1/self.k_dim)\n",
    "\n",
    "        # UP\n",
    "        j=len(self.downs)\n",
    "        for up, down, norm in zip(\n",
    "                self.ups, reversed(self.downs), self.up_norms\n",
    "        ):\n",
    "            # print(\"up conv...\")\n",
    "            #print(\"Kh:\",Kh.shape)\n",
    "            #print(\"Vh:\",Vh.shape)\n",
    "            assert j!=0, \"j should never be 0 in this case\"\n",
    "            (Kh, Vh, bh, eih, _) = history[j-1]\n",
    "            dK_scale = dK_scale * (down.ratio).pow(-1/self.k_dim)\n",
    "            V_ = up(Kh, K, Vh, V, edge_index, dK_scale=dK_scale)\n",
    "            if j>1 or not is_last:\n",
    "                # print(\"up norm V_ bh:\",V_.shape,bh.shape)\n",
    "                V_=norm(V_,bh)\n",
    "                V_=self.activation(V_)\n",
    "            # print(\"Vh:\",Vh.shape)\n",
    "            # print(\"V_:\",V_.shape)\n",
    "            V =Vh+up.alpha*V_\n",
    "            # print(\"history[j-1][1]:\",type(history[j-1][1]))\n",
    "            history[j-1][1]=V\n",
    "            K, edge_index = Kh, eih\n",
    "            j=j-1\n",
    "\n",
    "        return V, history\n",
    "\n",
    "class QM9Regressor(nn.Module):\n",
    "    @auto_save_hyperparams\n",
    "    def __init__(self,\n",
    "            domain_dim,\n",
    "            raw_feature_dim,\n",
    "            latent_feature_dim,\n",
    "            ratios,\n",
    "            mlp_dims_node: List[int],\n",
    "            mlp_dims_edge: List[int],\n",
    "            \n",
    "            aggr: str = 'sum',\n",
    "            \n",
    "            node_norm=nn.LayerNorm, \n",
    "            final_node_norm=nn.Identity, \n",
    "            \n",
    "            edge_norm=nn.LayerNorm,\n",
    "            final_edge_norm=nn.Identity, \n",
    "            \n",
    "            activation=nn.SiLU, \n",
    "            final_activation=nn.Identity, \n",
    "            \n",
    "            dropout_rate=0.1, \n",
    "            final_dropout_rate=0.0,\n",
    "\n",
    "            q=1,\n",
    "            n_encoders=3,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        assert mlp_dims_node[0]==mlp_dims_node[-1]==mlp_dims_edge[-1]==latent_feature_dim\n",
    "        self.atom_emb = nn.Embedding(200, latent_feature_dim)\n",
    "        self.unets=nn.ModuleList([])\n",
    "        for _ in range(n_encoders):\n",
    "            unet=UNet(\n",
    "                    ratios=ratios,\n",
    "                    mlp_dims_node=mlp_dims_node,\n",
    "                    mlp_dims_edge=mlp_dims_edge,\n",
    "                    aggr=aggr,\n",
    "                    node_norm=node_norm,\n",
    "                    final_node_norm=final_node_norm,\n",
    "                    edge_norm=edge_norm,\n",
    "                    final_edge_norm=final_edge_norm,\n",
    "                    activation=activation,\n",
    "                    final_activation=final_activation,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    final_dropout_rate=final_dropout_rate,\n",
    "                    q=q,\n",
    "                    k_dim=domain_dim,\n",
    "            )\n",
    "            self.unets.append(unet)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for unet in self.unets:\n",
    "            unet.reset_parameters()\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "        \n",
    "    def get_config(self):\n",
    "        return self.hparams\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self._forward(data.pos,data.atom_type,data.batch)\n",
    "    \n",
    "    def _forward(self, K, V, batch=None):\n",
    "        #K: float [N, 3] -> position\n",
    "        #V: int [N] -> atom type\n",
    "        #batch: int [N] -> minibatch id\n",
    "        V=self.atom_emb(V)\n",
    "        history=None\n",
    "        for i in range(len(self.unets)):\n",
    "            unet=self.unets[i]\n",
    "            is_last = (i==len(self.unets)-1)\n",
    "            V,history = unet(K, V, batch, history=history, is_last=is_last)\n",
    "        V=scatter(V, batch, dim=0, reduce='sum').mean(-1)\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1340557-fd4f-4d89-a542-d1e3ff505423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "models=[]\n",
    "models=[]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9293956f-b17c-4c38-87d2-4ce8999d41d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455809"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.schnet import SchNet\n",
    "\n",
    "models.append(TrainingContext(SchNet))\n",
    "model=models[-1]\n",
    "models[-1].num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b76395d-9a94-4887-8ce6-f15cfe821352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458260"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.m307b import M307B\n",
    "\n",
    "models.append(TrainingContext(\n",
    "    M307B,\n",
    "    node_dimses=[\n",
    "        [120, 150, 120],#this first element of the first nested list is arbitrary\n",
    "        [120, 150, 120],\n",
    "        [120, 150, 120],\n",
    "        [120, 150, 120],\n",
    "        [120, 150, 120],\n",
    "        [120, 150, 1 ],\n",
    "    ],\n",
    "    edge_dimses=[\n",
    "        [50, 220, 120],\n",
    "        [50, 220, 120],\n",
    "        [50, 220, 120],\n",
    "        [50, 220, 120],\n",
    "        [50, 220, 120],\n",
    "        [50, 220, 120],\n",
    "    ],\n",
    "    activation=nn.SiLU,\n",
    "    dropout_rate=0,#0.1,#0.01,\n",
    "))\n",
    "models[-1].num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16b432bd-b20d-41cc-9f8f-d00a0a29cf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584984"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM=1#scalar l norms\n",
    "EDGE_DIM=50\n",
    "EMB_DIM=120\n",
    "HDIM=64\n",
    "models.append(TrainingContext(QM9Regressor,\n",
    "    INPUT_DIM,EMB_DIM,HDIM,\n",
    "    [.25,.25,.25,.25],#downsample ratios\n",
    "    [HDIM,HDIM,HDIM],\n",
    "    [EDGE_DIM,HDIM*2,HDIM],#edge\n",
    "    q=5,\n",
    "    n_encoders=3,\n",
    "    ))\n",
    "model=models[-1]\n",
    "models[-1].num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e0bc8fc-737e-496d-897f-1f1d77ab7c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>9.814382</td>\n",
       "      <td>1809.465666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.554430</td>\n",
       "      <td>3.090360</td>\n",
       "      <td>3.835820</td>\n",
       "      <td>619867.683140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>1.406097</td>\n",
       "      <td>1.583795</td>\n",
       "      <td>0.337120</td>\n",
       "      <td>1.091630</td>\n",
       "      <td>1.369940</td>\n",
       "      <td>1.653980</td>\n",
       "      <td>437.903860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>1.124921</td>\n",
       "      <td>1.095618</td>\n",
       "      <td>0.331180</td>\n",
       "      <td>0.910480</td>\n",
       "      <td>1.078560</td>\n",
       "      <td>1.279540</td>\n",
       "      <td>282.945450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>2.706037</td>\n",
       "      <td>1.530394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588700</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.636100</td>\n",
       "      <td>29.556400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>75.191296</td>\n",
       "      <td>8.187793</td>\n",
       "      <td>6.310000</td>\n",
       "      <td>70.380000</td>\n",
       "      <td>75.500000</td>\n",
       "      <td>80.520000</td>\n",
       "      <td>196.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homo</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>-0.239977</td>\n",
       "      <td>0.022131</td>\n",
       "      <td>-0.428600</td>\n",
       "      <td>-0.252500</td>\n",
       "      <td>-0.241000</td>\n",
       "      <td>-0.228700</td>\n",
       "      <td>-0.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lumo</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>0.011124</td>\n",
       "      <td>0.046936</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>-0.023800</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gap</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.047519</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.216300</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.288200</td>\n",
       "      <td>0.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>1189.527450</td>\n",
       "      <td>279.757172</td>\n",
       "      <td>19.000200</td>\n",
       "      <td>1018.322600</td>\n",
       "      <td>1147.585800</td>\n",
       "      <td>1308.816600</td>\n",
       "      <td>3374.753200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zpve</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>0.148524</td>\n",
       "      <td>0.033274</td>\n",
       "      <td>0.015951</td>\n",
       "      <td>0.125289</td>\n",
       "      <td>0.148329</td>\n",
       "      <td>0.171150</td>\n",
       "      <td>0.273944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u0</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>-411.543985</td>\n",
       "      <td>40.060230</td>\n",
       "      <td>-714.568061</td>\n",
       "      <td>-437.913936</td>\n",
       "      <td>-417.864758</td>\n",
       "      <td>-387.049166</td>\n",
       "      <td>-40.478930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u298</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>-411.535513</td>\n",
       "      <td>40.060012</td>\n",
       "      <td>-714.560153</td>\n",
       "      <td>-437.905942</td>\n",
       "      <td>-417.857351</td>\n",
       "      <td>-387.039746</td>\n",
       "      <td>-40.476062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h298</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>-411.534569</td>\n",
       "      <td>40.060012</td>\n",
       "      <td>-714.559209</td>\n",
       "      <td>-437.904997</td>\n",
       "      <td>-417.856407</td>\n",
       "      <td>-387.038802</td>\n",
       "      <td>-40.475117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g298</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>-411.577397</td>\n",
       "      <td>40.060741</td>\n",
       "      <td>-714.602138</td>\n",
       "      <td>-437.947682</td>\n",
       "      <td>-417.895731</td>\n",
       "      <td>-387.083279</td>\n",
       "      <td>-40.498597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>31.600676</td>\n",
       "      <td>4.062471</td>\n",
       "      <td>6.002000</td>\n",
       "      <td>28.942000</td>\n",
       "      <td>31.555000</td>\n",
       "      <td>34.276000</td>\n",
       "      <td>46.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u0_atom</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>-1750.812997</td>\n",
       "      <td>239.313373</td>\n",
       "      <td>-2608.448864</td>\n",
       "      <td>-1904.772194</td>\n",
       "      <td>-1753.464714</td>\n",
       "      <td>-1596.871098</td>\n",
       "      <td>-213.087624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u298_atom</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>-1761.480647</td>\n",
       "      <td>241.436103</td>\n",
       "      <td>-2626.408172</td>\n",
       "      <td>-1916.981009</td>\n",
       "      <td>-1764.090324</td>\n",
       "      <td>-1606.108658</td>\n",
       "      <td>-213.974294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h298_atom</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>-1771.546928</td>\n",
       "      <td>243.151065</td>\n",
       "      <td>-2643.007040</td>\n",
       "      <td>-1928.181417</td>\n",
       "      <td>-1774.151176</td>\n",
       "      <td>-1615.066349</td>\n",
       "      <td>-215.159658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g298_atom</th>\n",
       "      <td>133885.0</td>\n",
       "      <td>-1629.388196</td>\n",
       "      <td>220.207088</td>\n",
       "      <td>-2417.121997</td>\n",
       "      <td>-1771.350603</td>\n",
       "      <td>-1632.224955</td>\n",
       "      <td>-1488.291333</td>\n",
       "      <td>-201.407171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count         mean          std          min          25%  \\\n",
       "A          133885.0     9.814382  1809.465666     0.000000     2.554430   \n",
       "B          133885.0     1.406097     1.583795     0.337120     1.091630   \n",
       "C          133885.0     1.124921     1.095618     0.331180     0.910480   \n",
       "mu         133885.0     2.706037     1.530394     0.000000     1.588700   \n",
       "alpha      133885.0    75.191296     8.187793     6.310000    70.380000   \n",
       "homo       133885.0    -0.239977     0.022131    -0.428600    -0.252500   \n",
       "lumo       133885.0     0.011124     0.046936    -0.175000    -0.023800   \n",
       "gap        133885.0     0.251100     0.047519     0.024600     0.216300   \n",
       "r2         133885.0  1189.527450   279.757172    19.000200  1018.322600   \n",
       "zpve       133885.0     0.148524     0.033274     0.015951     0.125289   \n",
       "u0         133885.0  -411.543985    40.060230  -714.568061  -437.913936   \n",
       "u298       133885.0  -411.535513    40.060012  -714.560153  -437.905942   \n",
       "h298       133885.0  -411.534569    40.060012  -714.559209  -437.904997   \n",
       "g298       133885.0  -411.577397    40.060741  -714.602138  -437.947682   \n",
       "cv         133885.0    31.600676     4.062471     6.002000    28.942000   \n",
       "u0_atom    133885.0 -1750.812997   239.313373 -2608.448864 -1904.772194   \n",
       "u298_atom  133885.0 -1761.480647   241.436103 -2626.408172 -1916.981009   \n",
       "h298_atom  133885.0 -1771.546928   243.151065 -2643.007040 -1928.181417   \n",
       "g298_atom  133885.0 -1629.388196   220.207088 -2417.121997 -1771.350603   \n",
       "\n",
       "                   50%          75%            max  \n",
       "A             3.090360     3.835820  619867.683140  \n",
       "B             1.369940     1.653980     437.903860  \n",
       "C             1.078560     1.279540     282.945450  \n",
       "mu            2.500000     3.636100      29.556400  \n",
       "alpha        75.500000    80.520000     196.620000  \n",
       "homo         -0.241000    -0.228700      -0.101700  \n",
       "lumo          0.012000     0.049200       0.193500  \n",
       "gap           0.249400     0.288200       0.622100  \n",
       "r2         1147.585800  1308.816600    3374.753200  \n",
       "zpve          0.148329     0.171150       0.273944  \n",
       "u0         -417.864758  -387.049166     -40.478930  \n",
       "u298       -417.857351  -387.039746     -40.476062  \n",
       "h298       -417.856407  -387.038802     -40.475117  \n",
       "g298       -417.895731  -387.083279     -40.498597  \n",
       "cv           31.555000    34.276000      46.969000  \n",
       "u0_atom   -1753.464714 -1596.871098    -213.087624  \n",
       "u298_atom -1764.090324 -1606.108658    -213.974294  \n",
       "h298_atom -1774.151176 -1615.066349    -215.159658  \n",
       "g298_atom -1632.224955 -1488.291333    -201.407171  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(-380.1364430652005), np.float64(226.3763836451994))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=pd.read_csv(\"datasets/qm9/raw/gdb9.sdf.csv\")\n",
    "display(df.describe().T)\n",
    "YCOL='u0'\n",
    "#(df[YCOL].quantile([.01,.02,.03,.05,.5,.95,.97,.98,.99]))\n",
    "__min=df[YCOL].quantile(.0001)\n",
    "__max=df[YCOL].quantile(.9999)\n",
    "__mean=df[YCOL].mean()\n",
    "__std=df[YCOL].std()\n",
    "df[YCOL].plot.hist(bins=200)\n",
    "offset=(__min+__max)/2\n",
    "width=abs(__max-__min)/2\n",
    "offset,width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e6deb-1579-40c8-bc02-982fc1c297c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0b8efe129e4cde9631da56c74bf416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/411603 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6222393e97a64390a99ada3699ae36d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "BATCH_SIZE=32\n",
    "N_EPOCHS=100\n",
    "N_mols=len(train_dataset)\n",
    "long_bar=tqdm(range(N_EPOCHS*N_mols//BATCH_SIZE),smoothing=0)\n",
    "e_bar=tqdm(range(N_mols//BATCH_SIZE),smoothing=0)\n",
    "Q=100000\n",
    "N_TRAINING_MODEL=len(models)\n",
    "for model in models:\n",
    "    model.inner=model.inner.train()\n",
    "    model.stopped=False\n",
    "    \n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(N_EPOCHS):\n",
    "    e_bar.refresh()\n",
    "    e_bar.reset(N_mols//BATCH_SIZE)\n",
    "    train_bl=DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for i, batch in zip(range(N_mols//BATCH_SIZE), train_bl):\n",
    "        long_bar.update(1)\n",
    "        e_bar.update(1)\n",
    "        try:\n",
    "            batch=(batch.to(cuda))\n",
    "            \n",
    "            for model in models:\n",
    "                if model.stopped or not model.training: continue\n",
    "                model.inner=model.inner.train()\n",
    "                out = model.inner(batch)\n",
    "                extra_loss=0\n",
    "                if isinstance(out, tuple):\n",
    "                    out, extra_loss=out\n",
    "                loss = ((out*__std+__mean-batch[YCOL])**2).mean()+extra_loss\n",
    "                model.inner.zero_grad(set_to_none=True)\n",
    "                \n",
    "                if model.total_iters<5:\n",
    "                    model.running_loss = model.running_loss*.3+loss.item()*.7\n",
    "                else:\n",
    "                    model.running_loss = model.running_loss*(1-1/100)+loss.item()/100\n",
    "                    \n",
    "                if model.total_iters+.2>1.1**len(model.train_loss_record):\n",
    "                    model.train_loss_record[model.total_iters]=model.running_loss\n",
    "                loss.backward()\n",
    "                model.optim.step()\n",
    "                model.total_iters+=1\n",
    "\n",
    "            ### evaluation\n",
    "            if i%32==0:\n",
    "                for model in models:\n",
    "                    if model.stopped or not model.training: continue\n",
    "                    model.inner=model.inner.eval()\n",
    "                    L=len(test_dataset)\n",
    "                    test_bl=DataLoader(test_dataset, batch_size=L)\n",
    "                    all_eval_y=list()\n",
    "                    model.test_loss_record[model.total_iters]=0\n",
    "                    for batch in test_bl:\n",
    "                        batch=(batch.to(cuda))\n",
    "                        all_eval_y+=batch[YCOL].view(-1).tolist()\n",
    "                        with torch.no_grad():\n",
    "                            out = model.inner(batch)#X, A, E, batch=b)\n",
    "                            if isinstance(out, tuple):\n",
    "                                out, _=out\n",
    "                        model.results=out.view(-1).tolist()\n",
    "                        loss = ((out*__std+__mean-batch[YCOL]).abs()).sum().detach().item()\n",
    "                        model.test_loss_record[model.total_iters]+=loss/L\n",
    "                    \n",
    "                    if model.test_loss_record[model.total_iters]<model.best_eval_loss:\n",
    "                        model.best_eval_loss=model.test_loss_record[model.total_iters]\n",
    "                        if model.total_iters>10000:\n",
    "                            model.save()\n",
    "                        #if np.log10(loss/models[model_name]['best_eval_loss'])>1.5 and models[model_name]['total_iters']>100:\n",
    "                        #    models[model_name]['stopped']=True\n",
    "                        #    N_TRAINING_MODEL-=1\n",
    "            ###logging\n",
    "            logstr=''\n",
    "            for model in models:\n",
    "                if model.stopped:\n",
    "                    logstr+=f'{model.name}(stopped):{int(Q*model.running_loss)/Q}({int(Q*model.best_eval_loss)/Q}). '\n",
    "                else:\n",
    "                    logstr+=f'{model.name}:{int(Q*model.running_loss)/Q}({int(Q*model.best_eval_loss)/Q}). '\n",
    "            e_bar.set_description(logstr)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    ### stepping schedulers:\n",
    "    \n",
    "    for model in models:\n",
    "        if model.sched is None: continue\n",
    "        if model.stopped or not model.training: continue\n",
    "        model.inner=model.inner.eval()\n",
    "        L=len(test_dataset)\n",
    "        test_bl=DataLoader(test_dataset, batch_size=L)\n",
    "        total_eval_loss=0\n",
    "        for batch in test_bl:\n",
    "            batch=(batch.to(cuda))\n",
    "            eval_y=(batch[YCOL]-__mean)/__std\n",
    "            with torch.no_grad():\n",
    "                out = model.inner(batch)#X, A, E, batch=b)\n",
    "                if isinstance(out, tuple):\n",
    "                    out, _=out\n",
    "            loss = ((out-eval_y).abs()).sum().detach().item()\n",
    "            total_eval_loss+=loss/L\n",
    "        model.sched.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89cc2713-f863-4ee2-a611-1477b3c7d3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QM9Regressor(\n",
       "  (atom_emb): Embedding(200, 64)\n",
       "  (unets): ModuleList(\n",
       "    (0-2): 3 x UNet(\n",
       "      (activation): SiLU()\n",
       "      (downs): ModuleList(\n",
       "        (0-3): 4 x DownSampler(\n",
       "          (gauss): GaussianSmearing()\n",
       "          (conv): ConvLayer()\n",
       "          (norm): GraphNorm(64)\n",
       "        )\n",
       "      )\n",
       "      (ups): ModuleList(\n",
       "        (0-3): 4 x UpSampler(\n",
       "          (conv): ConvLayer()\n",
       "          (gauss): GaussianSmearing()\n",
       "        )\n",
       "      )\n",
       "      (down_norms): ModuleList(\n",
       "        (0-3): 4 x GraphNorm(64)\n",
       "      )\n",
       "      (up_norms): ModuleList(\n",
       "        (0-3): 4 x GraphNorm(64)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "786c9f18-be11-42c4-b136-205744a382b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-52.6892, -37.7710, -69.0671, -37.8368, -45.1797, -59.7259, -51.5643,\n",
       "        -43.4150, -55.4387, -33.9428, -83.1115, -42.7684, -55.1416,  -8.2797,\n",
       "        -74.6718, -42.5277, -65.7413, -30.1682, -34.3769, -31.0287, -75.0142,\n",
       "        -54.4015, -42.6130,   0.4807, -34.4269, -59.5253, -22.2995, -40.7741,\n",
       "        -67.4187, -44.0374, -69.5282, -11.4092], device='cuda:0',\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b5cfe0f-d9a3-4c40-981e-1147b28c19ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-405.5599, -382.6054, -387.0468, -416.8701, -420.4649, -422.9843,\n",
       "        -439.0317, -440.3035, -421.7949, -476.2469, -352.3575, -436.7056,\n",
       "        -437.8705, -448.9181, -385.8275, -419.2387, -400.7570, -434.0748,\n",
       "        -439.1047, -362.5703, -384.5775, -440.3046, -437.9566, -489.8797,\n",
       "        -383.7417, -424.2003, -434.1044, -401.9162, -388.2587, -383.2213,\n",
       "        -367.2304, -455.1707], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.u0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241523b-8358-439a-b709-694dc2b3d261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
